{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Meta Kaggle and Meta Kaggle Code Exploratory Data Analysis\n",
    "\n",
    "![Kaggle Logo](data/kaggle_data/kaggle-logo.png)"
   ],
   "id": "715d382ddcf736e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "e6bd0e608255d65f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.) <u>Accelerators, Imports, and Datasets' Size Check</u>\n",
    "\n",
    "---"
   ],
   "id": "48d064a80a2a3ab4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.A) *GPU Acceleration*",
   "id": "5ffe2f0f6fdaf21c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T05:17:48.561198447Z",
     "start_time": "2026-01-31T05:17:22.517445926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GPU acceleration for pandas and sklearn\n",
    "%load_ext cudf.pandas\n",
    "%load_ext cuml.accel"
   ],
   "id": "ed5331992bd0847",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.B) *Python Library Imports*",
   "id": "698efda020704b72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T08:49:21.975999705Z",
     "start_time": "2026-01-31T08:49:21.620770861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# general imports\n",
    "from  pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# data science/scientific imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "\n",
    "# file conversion imports\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# machine learning imports\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import Normalizer, normalize, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# currency conversion import\n",
    "from currency_converter import CurrencyConverter as cc"
   ],
   "id": "d869fd0e45820b3d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.C) *Meta-Kaggle `.csv` Files' Size Check*\n",
    "- prints files in order from smallest to largest file size"
   ],
   "id": "2376c360fe51403a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T07:04:01.819285126Z",
     "start_time": "2026-01-31T07:03:53.984746368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "directory = Path('data/kaggle_data/meta-kaggle/')\n",
    "\n",
    "sorted_files = sorted(\n",
    "\t(f for f in directory.iterdir() if f.is_file()),\n",
    "    key=lambda x: x.stat().st_size\n",
    "    )\n",
    "for file in sorted_files:\n",
    "    size_in_mb = file.stat().st_size / (1 << 20)\n",
    "    print(f'{file.name}: {size_in_mb:.3f}MB')"
   ],
   "id": "76f1781b8eacd63d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KernelVotes.csv: 0.000MB\n",
      "KernelAcceleratorTypes.csv: 0.000MB\n",
      "KernelLanguages.csv: 0.000MB\n",
      "CompetitionTags.csv: 0.027MB\n",
      "Tags.csv: 0.096MB\n",
      "UserOrganizations.csv: 0.148MB\n",
      "Organizations.csv: 0.275MB\n",
      "ModelTags.csv: 0.280MB\n",
      "DatasetTaskSubmissions.csv: 0.622MB\n",
      "ModelVotes.csv: 2.263MB\n",
      "ModelVariations.csv: 2.862MB\n",
      "DatasetTasks.csv: 2.894MB\n",
      "Models.csv: 3.193MB\n",
      "ForumMessageReactions.csv: 8.569MB\n",
      "ModelVersions.csv: 11.474MB\n",
      "DatasetTags.csv: 17.163MB\n",
      "KernelVersionModelSources.csv: 17.786MB\n",
      "ModelVariationVersions.csv: 20.929MB\n",
      "KernelTags.csv: 27.628MB\n",
      "Forums.csv: 30.192MB\n",
      "Datasources.csv: 41.714MB\n",
      "KernelVersionKernelSources.csv: 42.528MB\n",
      "ForumTopics.csv: 64.760MB\n",
      "UserFollowers.csv: 76.268MB\n",
      "Datasets.csv: 83.611MB\n",
      "DatasetVotes.csv: 101.145MB\n",
      "Competitions.csv: 111.418MB\n",
      "KernelVersionCompetitionSources.csv: 152.390MB\n",
      "ForumMessageVotes.csv: 192.802MB\n",
      "Kernels.csv: 260.587MB\n",
      "KernelVersionDatasetSources.csv: 369.191MB\n",
      "TeamMemberships.csv: 379.493MB\n",
      "Teams.csv: 684.027MB\n",
      "DatasetVersions.csv: 1124.922MB\n",
      "ForumMessages.csv: 1556.266MB\n",
      "Users.csv: 2173.086MB\n",
      "Submissions.csv: 2225.732MB\n",
      "KernelVersions.csv: 4218.634MB\n",
      "Episodes.csv: 5065.967MB\n",
      "UserAchievements.csv: 8320.762MB\n",
      "EpisodeAgents.csv: 17358.091MB\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "b8a095cf8f86398d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.) <u>Data Loading</u>\n",
    "\n",
    "---"
   ],
   "id": "4e06ae7fc3776d5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.A) *Data Loading Strategy*\n",
    "\n",
    "**Strategy**: All CSV files used in this notebook are converted to **Parquet** format once (stored in `data/kaggle_data/meta-kaggle-parquet/`).<br><br>\n",
    "Parquet is a columnar binary format that provides:\n",
    "- **~10-20x faster reads** — no CSV parsing overhead\n",
    "- **~60-80% smaller on disk** — built-in compression\n",
    "- **Free column selection** — `columns=` only reads selected columns from disk<br><br>\n",
    "\n",
    "After the one-time conversion, every loading cell runs in seconds rather than minutes.\n",
    "\n",
    "| Size Category | Files | Approach |\n",
    "|---------------|---|---|\n",
    "| Small (<1MB)  | KernelLanguages, AcceleratorTypes, Tags, CompetitionTags | Direct CSV (fast enough) |\n",
    "| Medium–Huge   | All others | Read from parquet with `columns=` |"
   ],
   "id": "7e32889442e60b48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.B) *Reference Tables (from `.csv`) File Loading*",
   "id": "475c283d74544fac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T07:11:11.157145762Z",
     "start_time": "2026-01-31T07:11:07.143625716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# base path\n",
    "META_KAGGLE = Path('data/kaggle_data/meta-kaggle/')\n",
    "\n",
    "# ── tiny reference tables ──\n",
    "kernel_languages = pd.read_csv(META_KAGGLE / 'KernelLanguages.csv')\n",
    "accelerator_types = pd.read_csv(META_KAGGLE / 'KernelAcceleratorTypes.csv')\n",
    "tags = pd.read_csv(META_KAGGLE / 'Tags.csv')\n",
    "comp_tags = pd.read_csv(META_KAGGLE / 'CompetitionTags.csv')\n",
    "\n",
    "print(\"=== Reference Tables ===\")\n",
    "for name, df in [('KernelLanguages', kernel_languages), ('AcceleratorTypes', accelerator_types),\n",
    "                 ('Tags', tags), ('CompetitionTags', comp_tags)]:\n",
    "    print(f\"  {name}: {df.shape}  |  Memory: {df.memory_usage(deep=True).sum() / 1e6:.4f} MB\")"
   ],
   "id": "d325d6048b97fbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reference Tables ===\n",
      "  KernelLanguages: (10, 4)  |  Memory: 0.0004 MB\n",
      "  AcceleratorTypes: (14, 2)  |  Memory: 0.0003 MB\n",
      "  Tags: (831, 9)  |  Memory: 0.1071 MB\n",
      "  CompetitionTags: (1201, 3)  |  Memory: 0.0304 MB\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.C) *One-Time CSV → Parquet Conversion*\n",
    "\n",
    "- Run the cell below once to convert all large CSVs to parquet\n",
    "- It skips files that already exist, so re-running is safe\n",
    "- Very large files (>200 MB) are streamed in chunks to limit memory usage during conversion"
   ],
   "id": "c8e57f236fa74177"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Storage Configuration ──\n",
    "PARQUET_DIR = Path('data/kaggle_data/meta-kaggle-parquet/')\n",
    "RANDOM_STATE = 33"
   ],
   "id": "8c17bd990a5c8264"
  },
  {
   "cell_type": "code",
   "id": "zv6jhph78p",
   "source": [
    "PARQUET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CSVs used in this notebook\n",
    "_CONVERT = [\n",
    "    'Competitions', 'Kernels', 'KernelVersionCompetitionSources', 'KernelTags',\n",
    "    'Teams', 'TeamMemberships', 'KernelVersions', 'Users', 'Submissions',\n",
    "    'UserAchievements',\n",
    "]\n",
    "_CHUNK_THRESHOLD_MB = 2000\n",
    "\n",
    "\n",
    "def _convert_csv_to_parquet(name):\n",
    "    \"\"\"Convert a single CSV to parquet. Returns (name, status_message).\"\"\"\n",
    "    parquet_path = PARQUET_DIR / f'{name}.parquet'\n",
    "    csv_path = META_KAGGLE / f'{name}.csv'\n",
    "\n",
    "    if parquet_path.exists():\n",
    "        pq_mb = parquet_path.stat().st_size / (1 << 20)\n",
    "        return (name, f\"{pq_mb:.3f} MB — already exists, skipping\")\n",
    "\n",
    "    csv_mb = csv_path.stat().st_size / (1 << 20)\n",
    "\n",
    "    if csv_mb > _CHUNK_THRESHOLD_MB:\n",
    "        # Stream chunks to parquet to limit peak memory\n",
    "        writer = None\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=1000000):\n",
    "            table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(parquet_path, table.schema)\n",
    "            writer.write_table(table)\n",
    "        if writer:\n",
    "            writer.close()\n",
    "    else:\n",
    "        pd.read_csv(csv_path).to_parquet(parquet_path, index=False)\n",
    "\n",
    "    pq_mb = parquet_path.stat().st_size / (1 << 20)\n",
    "    return (name, f\"{csv_mb:.0f} MB CSV -> {pq_mb:.3f} MB parquet ({pq_mb / csv_mb * 100:.3f}%)\")\n",
    "\n",
    "\n",
    "# Use 5 workers to keep disk I/O saturated without thrashing\n",
    "num_workers = min(5, cpu_count())\n",
    "print(f\"Converting with {num_workers} parallel workers...\\n\")\n",
    "\n",
    "with Pool(num_workers) as pool:\n",
    "    results = pool.map(_convert_csv_to_parquet, _CONVERT)\n",
    "\n",
    "for name, msg in results:\n",
    "    print(f\"  {name}: {msg}\")\n",
    "\n",
    "print(\"\\nConversion complete.\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.D) *Medium Tables (from `.parquet`) File Loading*",
   "id": "bf5a55318648f56e"
  },
  {
   "cell_type": "code",
   "id": "6b4558113bd546a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T05:31:07.256009075Z",
     "start_time": "2026-01-31T05:20:59.395803063Z"
    }
   },
   "source": "# ── medium tables (from parquet) ──\ncompetitions = pd.read_parquet(PARQUET_DIR / 'Competitions.parquet', columns=[\n    'Id', 'Title', 'EnabledDate', 'DeadlineDate', 'MaxTeamSize',\n    'RewardType', 'RewardQuantity', 'NumPrizes', 'TotalTeams',\n    'TotalCompetitors', 'TotalSubmissions', 'HostName', 'OnlyAllowKernelSubmissions'])\ncompetitions['EnabledDate'] = pd.to_datetime(competitions['EnabledDate'], format='mixed', errors='coerce')\ncompetitions['DeadlineDate'] = pd.to_datetime(competitions['DeadlineDate'], format='mixed', errors='coerce')\n\nkernels = pd.read_parquet(PARQUET_DIR / 'Kernels.parquet', columns=[\n    'Id', 'AuthorUserId', 'CurrentKernelVersionId', 'CreationDate', 'Medal', 'TotalVotes', 'TotalViews'])\nkernels['CreationDate'] = pd.to_datetime(kernels['CreationDate'], format='mixed', errors='coerce')\n\nkv_comp_sources = pd.read_parquet(PARQUET_DIR / 'KernelVersionCompetitionSources.parquet')\nkernel_tags = pd.read_parquet(PARQUET_DIR / 'KernelTags.parquet')\n\nprint(\"=== Medium Tables ===\")\nfor name, df in [('Competitions', competitions), ('Kernels', kernels),\n                 ('KVCompSources', kv_comp_sources), ('KernelTags', kernel_tags)]:\n    print(f\"  {name}: {df.shape}  |  Memory: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.E) *Large Tables (from `.parquet`) File Loading*",
   "id": "2331da2d04fec0c8"
  },
  {
   "cell_type": "code",
   "id": "33c14fd2b0ad42ef",
   "metadata": {},
   "source": "# ── large tables (from parquet) ──\nteams = pd.read_parquet(PARQUET_DIR / 'Teams.parquet', columns=[\n    'Id', 'CompetitionId', 'TeamLeaderId', 'TeamName', 'Medal',\n    'PublicLeaderboardRank', 'PrivateLeaderboardRank'])\n\nteam_memberships = pd.read_parquet(PARQUET_DIR / 'TeamMemberships.parquet')\n\nkernel_versions = pd.read_parquet(PARQUET_DIR / 'KernelVersions.parquet', columns=[\n    'Id', 'ScriptId', 'ScriptLanguageId', 'AuthorUserId', 'CreationDate',\n    'AcceleratorTypeId', 'TotalLines', 'RunningTimeInMilliseconds'])\nkernel_versions['CreationDate'] = pd.to_datetime(\n    kernel_versions['CreationDate'], format='mixed', errors='coerce')\n\nusers = pd.read_parquet(PARQUET_DIR / 'Users.parquet', columns=[\n    'Id', 'UserName', 'DisplayName', 'RegisterDate', 'PerformanceTier'])\nusers['RegisterDate'] = pd.to_datetime(users['RegisterDate'], format='mixed', errors='coerce')\n\nsubmissions = pd.read_parquet(PARQUET_DIR / 'Submissions.parquet', columns=[\n    'Id', 'SubmittedUserId', 'TeamId', 'SourceKernelVersionId', 'SubmissionDate'])\nsubmissions['SubmissionDate'] = pd.to_datetime(\n    submissions['SubmissionDate'], format='mixed', errors='coerce')\n\nprint(\"Large table summary:\")\nfor name, df in [('Teams', teams), ('TeamMemberships', team_memberships),\n                 ('KernelVersions', kernel_versions), ('Users', users),\n                 ('Submissions', submissions)]:\n    print(f\"  {name}: {df.shape}  |  Memory: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.F) *Extra-Large Tables (from `.parquet`) File Loading*",
   "id": "e44bf94714cc30b8"
  },
  {
   "cell_type": "code",
   "id": "7cc640ad93f649af",
   "metadata": {},
   "source": "# ── UserAchievements (from parquet, filter to Competitions only) ──\n# No date filter — achievement data is cumulative and has no date column.\nuser_achievements = pd.read_parquet(\n    PARQUET_DIR / 'UserAchievements.parquet',\n    columns=['UserId', 'AchievementType', 'Tier',\n             'TotalGold', 'TotalSilver', 'TotalBronze',\n             'Points', 'HighestRanking'])\n\n_before = len(user_achievements)\nuser_achievements = user_achievements[\n    user_achievements['AchievementType'] == 'Competitions'].reset_index(drop=True)\n\nprint(f\"UserAchievements: {_before:,} -> {len(user_achievements):,} rows (Competitions only)\")\nprint(f\"  Memory: {user_achievements.memory_usage(deep=True).sum() / 1e6:.1f} MB\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f48fe47df364e66",
   "metadata": {},
   "source": [
    "#### Data Loading — Quick Schema Reference\n",
    "\n",
    "| DataFrame | Rows (approx) | Key Columns | Joins To |\n",
    "|---|---------------|---|---|\n",
    "| `competitions` | ~1K           | Id, EnabledDate, RewardType, MaxTeamSize | Teams(CompetitionId), CompetitionTags(CompetitionId) |\n",
    "| `teams` | ~millions     | Id, CompetitionId, TeamLeaderId, Medal, Ranks | TeamMemberships(TeamId), Submissions(TeamId), Users(TeamLeaderId) |\n",
    "| `team_memberships` | ~millions     | TeamId, UserId | Teams(Id), Users(Id) |\n",
    "| `kernel_versions` | ~40+ millions | Id, ScriptId, ScriptLanguageId, AcceleratorTypeId | KernelLanguages, AcceleratorTypes, KVCompSources |\n",
    "| `users` | ~20+ millions | Id, PerformanceTier, RegisterDate | Teams, Kernels, Submissions |\n",
    "| `submissions` | ~20+ millions | TeamId, SourceKernelVersionId | Teams, KernelVersions |\n",
    "| `user_achievements` | ~80+ millions | UserId, Tier, TotalGold/Silver/Bronze | Users(Id) |"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "463500841dde131f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.) <u>Meta-Kaggle Exploratory Data Analysis (EDA)</u>\n",
    "\n",
    "---"
   ],
   "id": "7d42de22fc2e8353"
  },
  {
   "cell_type": "markdown",
   "id": "4d4b21402e114211",
   "metadata": {},
   "source": [
    "### 3.A) *Competition Landscape Overview*\n",
    "\n",
    "Before diving into the specific research questions, let's understand the overall Kaggle competition ecosystem:\n",
    "- how many competitions have been launched over time\n",
    "- what kinds of rewards they offer\n",
    "- how large they are\n",
    "- what subject areas they cover"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.A.i) ***Competitions Per Year***",
   "id": "9024b244f49f4ef1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# competitions launched per year\n",
    "comp_by_year = competitions.copy()\n",
    "comp_by_year['Year'] = comp_by_year['EnabledDate'].dt.year\n",
    "yearly_comps = comp_by_year.groupby('Year').size().reset_index(name='Count')\n",
    "yearly_comps = yearly_comps[yearly_comps['Year'].between(2010, 2025)]\n",
    "\n",
    "fig = px.bar(yearly_comps, x='Year', y='Count',\n",
    "             title='Number of Kaggle Competitions Launched Per Year',\n",
    "             labels={'Count': 'Number of Competitions'},\n",
    "             color_discrete_sequence=['#20BEFF'])\n",
    "fig.update_layout(width=1000, height=500, title_x=0.5)\n",
    "fig.show()"
   ],
   "id": "e0bdfcc8ee8fe557"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.A.ii) ***Competition Reward Types***",
   "id": "e4db54f483efd8c1"
  },
  {
   "cell_type": "code",
   "id": "eabc84c00b56440a",
   "metadata": {},
   "source": [
    "# reward type distribution\n",
    "reward_counts = competitions['RewardType'].value_counts().reset_index()\n",
    "reward_counts.columns = ['RewardType', 'Count']\n",
    "\n",
    "fig = px.pie(reward_counts, names='RewardType', values='Count',\n",
    "             title='Distribution of Competition Reward Types',\n",
    "             color_discrete_sequence=px.colors.qualitative.Set2)\n",
    "fig.update_layout(width=800, height=500, title_x=0.5)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.A.iii) ***Competition Cash Reward Sizes (USD)***",
   "id": "c7c7d8fd362ad271"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Filter to cash prize competitions and convert to USD\nconverter = cc()\n\n# Get competitions with cash rewards (filter out Knowledge, Jobs, etc.)\ncash_comps = competitions[\n    (competitions['RewardType'].str.contains('USD|EUR|GBP|INR|Prize', case=False, na=False)) &\n    (competitions['RewardQuantity'].notna()) &\n    (competitions['RewardQuantity'] > 0)\n].copy()\n\n# Extract currency and amount from RewardType\n# RewardType format examples: \"USD $100,000\", \"EUR €50,000\", \"Prize\"\ndef extract_currency_and_amount(row):\n    \"\"\"Extract currency code and convert reward to USD.\"\"\"\n    reward_type = str(row['RewardType'])\n    amount = row['RewardQuantity']\n    \n    # Default to USD if not specified or if it's just \"Prize\"\n    if 'EUR' in reward_type or '€' in reward_type:\n        currency = 'EUR'\n    elif 'GBP' in reward_type or '£' in reward_type:\n        currency = 'GBP'\n    elif 'INR' in reward_type or '₹' in reward_type:\n        currency = 'INR'\n    else:\n        currency = 'USD'  # Default assumption\n    \n    # Convert to USD\n    try:\n        if currency == 'USD':\n            return amount\n        else:\n            return converter.convert(amount, currency, 'USD')\n    except:\n        # If conversion fails, assume it's already USD\n        return amount\n\ncash_comps['RewardUSD'] = cash_comps.apply(extract_currency_and_amount, axis=1)\n\n# Create histogram\nfig = px.histogram(\n    cash_comps,\n    x='RewardUSD',\n    nbins=40,\n    title='Distribution of Competition Cash Rewards (USD)',\n    labels={'RewardUSD': 'Total Prize Pool (USD)'},\n    color_discrete_sequence=['#20BEFF']\n)\nfig.update_layout(\n    width=1000,\n    height=500,\n    title_x=0.5,\n    xaxis_title='Total Prize Pool (USD)',\n    yaxis_title='Number of Competitions',\n    xaxis_tickformat='$,.0f'\n)\nfig.show()\n\nprint(f\"\\nCash Prize Competition Statistics (n={len(cash_comps):,}):\")\nprint(cash_comps['RewardUSD'].describe().apply(lambda x: f'${x:,.0f}'))",
   "id": "56bfbd0529c578db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.A.iv) ***Competition Sizes (Total Competitors)***",
   "id": "f2a0b21d941ffa4e"
  },
  {
   "cell_type": "code",
   "id": "943c9722390847cf",
   "metadata": {},
   "source": [
    "# competition size distribution\n",
    "fig = px.histogram(competitions, x='TotalCompetitors',\n",
    "                   nbins=50,\n",
    "                   title='Distribution of Competition Sizes (by Total Competitors)',\n",
    "                   labels={'TotalCompetitors': 'Total Competitors'},\n",
    "                   color_discrete_sequence=['#20BEFF'])\n",
    "fig.update_layout(width=1000, height=500, title_x=0.5,\n",
    "                  xaxis_title='Total Competitors',\n",
    "                  yaxis_title='Number of Competitions')\n",
    "fig.show()\n",
    "\n",
    "print(\"Competition Size Summary Statistics:\")\n",
    "print(competitions[['TotalTeams', 'TotalCompetitors', 'TotalSubmissions']].describe().round(1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.A.v) ***Competition Cash Reward (USD) vs. Total Competitors***",
   "id": "9beba184cdaab6a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Scatter plot: Total Prize Pool (USD) vs. Total Competitors\n# Filter to competitions with both cash rewards and competitor data\nscatter_data = cash_comps[\n    (cash_comps['TotalCompetitors'].notna()) &\n    (cash_comps['TotalCompetitors'] > 0)\n].copy()\n\n# Add reward tier categories for color coding\ndef categorize_reward(amount):\n    if amount >= 1_000_000:\n        return '$1M+'\n    elif amount >= 100_000:\n        return '$100K-$1M'\n    elif amount >= 10_000:\n        return '$10K-$100K'\n    else:\n        return '<$10K'\n\nscatter_data['RewardTier'] = scatter_data['RewardUSD'].apply(categorize_reward)\n\n# Create scatter plot with log scale for better visualization\nfig = px.scatter(\n    scatter_data,\n    x='RewardUSD',\n    y='TotalCompetitors',\n    color='RewardTier',\n    title='Competition Prize Pool (USD) vs. Total Competitors',\n    labels={\n        'RewardUSD': 'Total Prize Pool (USD)',\n        'TotalCompetitors': 'Total Competitors',\n        'RewardTier': 'Prize Tier'\n    },\n    color_discrete_sequence=px.colors.qualitative.Bold,\n    category_orders={'RewardTier': ['<$10K', '$10K-$100K', '$100K-$1M', '$1M+']},\n    hover_data={\n        'Title': True,\n        'RewardUSD': ':$,.0f',\n        'TotalCompetitors': ':,',\n        'RewardTier': False\n    }\n)\n\n# Use log scale for both axes to better show the relationship\nfig.update_xaxes(type='log', tickformat='$,.0f')\nfig.update_yaxes(type='log', tickformat=',')\n\nfig.update_layout(\n    width=1000,\n    height=600,\n    title_x=0.5,\n    hovermode='closest'\n)\n\nfig.show()\n\n# Calculate correlation\nfrom scipy.stats import spearmanr\ncorr, p_value = spearmanr(scatter_data['RewardUSD'], scatter_data['TotalCompetitors'])\nprint(f\"\\nSpearman Correlation (Prize Pool vs. Competitors): {corr:.3f} (p={p_value:.4f})\")\nprint(f\"Number of cash prize competitions with competitor data: {len(scatter_data):,}\")",
   "id": "ad200e583db3c40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.A.vi) ***Competition Categories (via Tags)***",
   "id": "3007e7f923abf199"
  },
  {
   "cell_type": "code",
   "id": "5794a1424b7a4700",
   "metadata": {},
   "source": [
    "# top competition categories via tags\n",
    "comp_tag_names = comp_tags.merge(\n",
    "    tags[['Id', 'Name', 'FullPath']], left_on='TagId', right_on='Id', suffixes=('', '_tag'))\n",
    "top_comp_tags = comp_tag_names['Name'].value_counts().head(20).reset_index()\n",
    "top_comp_tags.columns = ['Tag', 'Count']\n",
    "\n",
    "fig = px.bar(top_comp_tags, x='Count', y='Tag', orientation='h',\n",
    "             title='Top 20 Competition Tags',\n",
    "             color_discrete_sequence=['#FF6F61'])\n",
    "fig.update_layout(width=1000, height=600, title_x=0.5,\n",
    "                  yaxis={'categoryorder': 'total ascending'})\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "vxuukcoloz",
   "source": "#### 3.A.vii) ***Medal Rate by Competition Size — Are Smaller Competitions More Winnable?***",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "w4hfd0zjdvf",
   "source": "# Medal rate by competition size\nteams_comp = teams.merge(\n    competitions[['Id', 'TotalCompetitors']],\n    left_on='CompetitionId', right_on='Id', suffixes=('', '_comp'))\nteams_comp = teams_comp[teams_comp['TotalCompetitors'].notna()].copy()\n\nteams_comp['SizeBin'] = pd.cut(\n    teams_comp['TotalCompetitors'],\n    bins=[0, 100, 500, 1000, 2000, 5000, 100_000],\n    labels=['<100', '100-500', '500-1K', '1K-2K', '2K-5K', '5K+'])\nteams_comp['HasMedal'] = teams_comp['Medal'].notna() & (teams_comp['Medal'] != '')\n\nrate_by_size = teams_comp.groupby('SizeBin', observed=True).agg(\n    TotalTeams=('HasMedal', 'count'),\n    MedalTeams=('HasMedal', 'sum')\n).reset_index()\nrate_by_size['MedalRate'] = (rate_by_size['MedalTeams'] / rate_by_size['TotalTeams'] * 100).round(2)\n\nfig = px.bar(rate_by_size, x='SizeBin', y='MedalRate', text='MedalRate',\n             title='Medal Rate by Competition Size (% of Teams Earning Any Medal)',\n             labels={'SizeBin': 'Competition Size (Total Competitors)', 'MedalRate': 'Medal Rate (%)'},\n             color_discrete_sequence=['#20BEFF'])\nfig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\nfig.update_layout(width=1000, height=500, title_x=0.5)\nfig.show()\n\nprint(\"Medal rate by competition size:\")\nprint(rate_by_size.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c4985661df2b48ca",
   "metadata": {},
   "source": [
    "### 3.B) Coding Languages & Hardware (Research Questions 1 & 5)\n",
    "\n",
    "> *What coding languages and code libraries are most often used?*\n",
    "> *What types of hardware are being used, and do winners most often use professional/corporate level hardware or is average enthusiast hardware enough?*\n",
    "\n",
    "We'll look at:\n",
    "- Overall language distribution across all Kaggle notebooks/scripts\n",
    "- Language trends over time\n",
    "- Language distribution for **competition-linked** kernels specifically\n",
    "- Hardware accelerator (GPU/TPU) usage and trends\n",
    "- Accelerator usage in competition kernels"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.B.i) ***Overall Coding-Language Distribution***",
   "id": "499149c763886af3"
  },
  {
   "cell_type": "code",
   "id": "facfe025fedd4c49",
   "metadata": {},
   "source": [
    "# overall language distribution across ALL kernel versions\n",
    "lang_dist = kernel_versions.merge(\n",
    "    kernel_languages, left_on='ScriptLanguageId', right_on='Id', suffixes=('', '_lang'))\n",
    "lang_counts = lang_dist['DisplayName'].value_counts().reset_index()\n",
    "lang_counts.columns = ['Language', 'Count']\n",
    "\n",
    "fig = px.pie(lang_counts, names='Language', values='Count',\n",
    "             title='Programming Language Distribution — All Kaggle Notebooks & Scripts',\n",
    "             color_discrete_sequence=px.colors.qualitative.Bold)\n",
    "fig.update_layout(width=800, height=500, title_x=0.5)\n",
    "fig.show()\n",
    "\n",
    "print(\"Language counts:\")\n",
    "print(lang_counts.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.B.ii) ***Coding-Language Usage (over time)***",
   "id": "7bace2ad0b853c35"
  },
  {
   "cell_type": "code",
   "id": "aaab3e3658704851",
   "metadata": {},
   "source": [
    "# language usage trends over time\n",
    "lang_dist['Year'] = lang_dist['CreationDate'].dt.year\n",
    "lang_yearly = lang_dist.groupby(['Year', 'DisplayName']).size().reset_index(name='Count')\n",
    "lang_yearly = lang_yearly[lang_yearly['Year'].between(2015, 2025)]\n",
    "\n",
    "fig = px.line(lang_yearly, x='Year', y='Count', color='DisplayName',\n",
    "              title='Programming Language Usage Over Time',\n",
    "              labels={'Count': 'Number of Kernel Versions', 'DisplayName': 'Language'},\n",
    "              markers=True)\n",
    "fig.update_layout(width=1000, height=500, title_x=0.5)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.B.iii) ***Coding-Language Distribution for Competition-Linked Kernels***",
   "id": "12549084db495ae"
  },
  {
   "cell_type": "code",
   "id": "6528bf8321074c6d",
   "metadata": {},
   "source": [
    "# language distribution for COMPETITION-LINKED kernels only\n",
    "comp_kv_ids = set(kv_comp_sources['KernelVersionId'].unique())\n",
    "comp_kernel_versions = kernel_versions[kernel_versions['Id'].isin(comp_kv_ids)]\n",
    "\n",
    "print(f\"Total kernel versions: {len(kernel_versions):,}\")\n",
    "print(f\"Competition-linked kernel versions: {len(comp_kernel_versions):,}\")\n",
    "print(f\"Percentage linked to competitions: {len(comp_kernel_versions)/len(kernel_versions)*100:.2f}%\")\n",
    "\n",
    "comp_lang = comp_kernel_versions.merge(\n",
    "    kernel_languages, left_on='ScriptLanguageId', right_on='Id', suffixes=('', '_lang'))\n",
    "comp_lang_counts = comp_lang['DisplayName'].value_counts().reset_index()\n",
    "comp_lang_counts.columns = ['Language', 'Count']\n",
    "\n",
    "fig = px.pie(comp_lang_counts, names='Language', values='Count',\n",
    "             title='Language Distribution — Competition-Linked Kernels Only',\n",
    "             color_discrete_sequence=px.colors.qualitative.Bold)\n",
    "fig.update_layout(width=800, height=500, title_x=0.5)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nCompetition-linked kernel language counts:\")\n",
    "print(comp_lang_counts.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.B.iv) ***Hardware Accelerator Usage***",
   "id": "76cc7d5f89b45329"
  },
  {
   "cell_type": "code",
   "id": "c4d6cd3337704e91",
   "metadata": {},
   "source": [
    "# hardware accelerator usage — all kernels\n",
    "accel_dist = kernel_versions.merge(\n",
    "    accelerator_types, left_on='AcceleratorTypeId', right_on='Id', suffixes=('', '_accel'))\n",
    "accel_counts = accel_dist['Label'].value_counts().reset_index()\n",
    "accel_counts.columns = ['Accelerator', 'Count']\n",
    "\n",
    "fig = px.bar(accel_counts, x='Count', y='Accelerator', orientation='h',\n",
    "             title='Hardware Accelerator Usage Across All Kernels',\n",
    "             color_discrete_sequence=['#6C5B7B'])\n",
    "fig.update_layout(width=1000, height=400, title_x=0.5,\n",
    "                  yaxis={'categoryorder': 'total ascending'})\n",
    "fig.show()\n",
    "\n",
    "# percentage breakdown\n",
    "accel_counts['Pct'] = (accel_counts['Count'] / accel_counts['Count'].sum() * 100).round(2)\n",
    "print(\"Accelerator breakdown:\")\n",
    "print(accel_counts.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.B.v) ***Hardware Accelerator Adoption (over time; all kernels)***",
   "id": "4cfe5098346cfe91"
  },
  {
   "cell_type": "code",
   "id": "ab3a91ee3b554c5a",
   "metadata": {},
   "source": [
    "# GPU/TPU accelerator adoption over time (excluding \"None\")\n",
    "accel_dist['Year'] = accel_dist['CreationDate'].dt.year\n",
    "accel_yearly = (accel_dist[accel_dist['Label'] != 'None']\n",
    "                .groupby(['Year', 'Label']).size()\n",
    "                .reset_index(name='Count'))\n",
    "accel_yearly = accel_yearly[accel_yearly['Year'].between(2015, 2025)]\n",
    "\n",
    "fig = px.line(accel_yearly, x='Year', y='Count', color='Label',\n",
    "              title='GPU / TPU Accelerator Adoption Over Time (Excluding \"None\")',\n",
    "              labels={'Count': 'Number of Kernel Versions', 'Label': 'Accelerator'},\n",
    "              markers=True)\n",
    "fig.update_layout(width=1000, height=500, title_x=0.5)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.B.iv) ***Hardware Accelerator Usage for Competition-Linked Kernels***",
   "id": "ca61c9a02cf6aee7"
  },
  {
   "cell_type": "code",
   "id": "9db2b41282c044a9",
   "metadata": {},
   "source": [
    "# accelerator usage in competition-linked kernels specifically\n",
    "comp_accel = comp_kernel_versions.merge(\n",
    "    accelerator_types, left_on='AcceleratorTypeId', right_on='Id', suffixes=('', '_accel'))\n",
    "comp_accel_counts = comp_accel['Label'].value_counts().reset_index()\n",
    "comp_accel_counts.columns = ['Accelerator', 'Count']\n",
    "comp_accel_counts['Pct'] = (comp_accel_counts['Count'] / comp_accel_counts['Count'].sum() * 100).round(2)\n",
    "\n",
    "fig = px.pie(comp_accel_counts, names='Accelerator', values='Count',\n",
    "             title='Accelerator Usage in Competition-Linked Kernels',\n",
    "             color_discrete_sequence=px.colors.qualitative.Pastel)\n",
    "fig.update_layout(width=800, height=500, title_x=0.5)\n",
    "fig.show()\n",
    "\n",
    "print(\"Competition kernel accelerator breakdown:\")\n",
    "print(comp_accel_counts.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ckbva0hg5ub",
   "source": "#### 3.B.vii) ***Compute Intensity vs. Competition Performance — Does Hardware Decide Winners?***\n\nDo medal-winning competition submissions require longer runtimes, more code, or more powerful accelerators than non-winning ones? If not, clever feature engineering may matter more than raw compute.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8xa7y8jqbcw",
   "source": "# Link competition-linked kernel versions to team medal outcomes via submissions\nkernel_subs = submissions[submissions['SourceKernelVersionId'].notna()][\n    ['SourceKernelVersionId', 'TeamId']].drop_duplicates()\n\ncomp_kv_outcomes = comp_kernel_versions.merge(\n    kernel_subs, left_on='Id', right_on='SourceKernelVersionId', how='inner')\ncomp_kv_outcomes = comp_kv_outcomes.merge(\n    teams[['Id', 'Medal']].rename(columns={'Id': 'TeamId_teams'}),\n    left_on='TeamId', right_on='TeamId_teams', how='left')\n\ncomp_kv_outcomes['Outcome'] = comp_kv_outcomes['Medal'].apply(\n    lambda x: 'Medal Winner' if pd.notna(x) and str(x).strip() not in ('', 'nan') else 'No Medal')\n\n# Filter to kernels with valid runtime and code length\nvalid_kv = comp_kv_outcomes[\n    (comp_kv_outcomes['RunningTimeInMilliseconds'] > 0) &\n    (comp_kv_outcomes['TotalLines'] > 0)].copy()\nvalid_kv['RuntimeMinutes'] = valid_kv['RunningTimeInMilliseconds'] / 60_000\n\n# Runtime comparison\nfig = px.box(valid_kv, x='Outcome', y='RuntimeMinutes', color='Outcome',\n             title='Kernel Runtime: Medal Winners vs. Non-Medal (Competition Submissions)',\n             labels={'RuntimeMinutes': 'Runtime (minutes)', 'Outcome': ''},\n             color_discrete_map={'Medal Winner': '#FFD700', 'No Medal': '#B0BEC5'})\nfig.update_yaxes(type='log', title_text='Runtime (minutes, log scale)')\nfig.update_layout(width=1000, height=500, title_x=0.5, showlegend=False)\nfig.show()\n\n# Code length comparison\nfig = px.box(valid_kv, x='Outcome', y='TotalLines', color='Outcome',\n             title='Code Length: Medal Winners vs. Non-Medal (Competition Submissions)',\n             labels={'TotalLines': 'Total Lines of Code', 'Outcome': ''},\n             color_discrete_map={'Medal Winner': '#FFD700', 'No Medal': '#B0BEC5'})\nfig.update_yaxes(type='log', title_text='Total Lines of Code (log scale)')\nfig.update_layout(width=1000, height=500, title_x=0.5, showlegend=False)\nfig.show()\n\n# Summary statistics\nprint(f\"Kernels analyzed: {len(valid_kv):,}\\n\")\nfor outcome in ['Medal Winner', 'No Medal']:\n    subset = valid_kv[valid_kv['Outcome'] == outcome]\n    print(f\"  {outcome} (n={len(subset):,}):\")\n    print(f\"    Runtime — median: {subset['RuntimeMinutes'].median():.1f} min, \"\n          f\"mean: {subset['RuntimeMinutes'].mean():.1f} min\")\n    print(f\"    Code    — median: {subset['TotalLines'].median():.0f} lines, \"\n          f\"mean: {subset['TotalLines'].mean():.0f} lines\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bflj3ifem0g",
   "source": "# Accelerator usage: medal winners vs. non-medal in competition submissions\naccel_outcomes = comp_kv_outcomes.merge(\n    accelerator_types, left_on='AcceleratorTypeId', right_on='Id', suffixes=('', '_accel'))\n\n# Compute percentage within each outcome group\naccel_pcts = accel_outcomes.groupby(['Outcome', 'Label']).size().reset_index(name='Count')\ntotals = accel_pcts.groupby('Outcome')['Count'].transform('sum')\naccel_pcts['Pct'] = (accel_pcts['Count'] / totals * 100).round(2)\n\nfig = px.bar(accel_pcts, x='Label', y='Pct', color='Outcome',\n             barmode='group',\n             title='Accelerator Usage: Medal Winners vs. Non-Medal (Competition Submissions)',\n             labels={'Label': 'Accelerator', 'Pct': 'Percentage within Group (%)'},\n             color_discrete_map={'Medal Winner': '#FFD700', 'No Medal': '#B0BEC5'})\nfig.update_layout(width=1000, height=500, title_x=0.5,\n                  xaxis={'categoryorder': 'total descending'})\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "319eba8314214263",
   "metadata": {},
   "source": [
    "### 3.C) *Individuals vs. Teams (Research Question 3)*\n",
    "\n",
    "> *Are competition winners most often individuals or teams?*\n",
    "\n",
    "- We compute team sizes by counting memberships per team, then analyze how solo competitors compare to multi-member teams in terms of participation rates and medal outcomes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.C.i)",
   "id": "fbb234341ce0c4d8"
  },
  {
   "cell_type": "code",
   "id": "77057c9ef4474da7",
   "metadata": {},
   "source": [
    "# compute team sizes\n",
    "team_sizes = team_memberships.groupby('TeamId').size().reset_index(name='TeamSize')\n",
    "teams_with_size = teams.merge(team_sizes, left_on='Id', right_on='TeamId', how='left')\n",
    "teams_with_size['TeamSize'] = teams_with_size['TeamSize'].fillna(1).astype(int)\n",
    "teams_with_size['IsSolo'] = teams_with_size['TeamSize'] == 1\n",
    "\n",
    "print(f\"Total teams: {len(teams_with_size):,}\")\n",
    "print(f\"Solo competitors: {teams_with_size['IsSolo'].sum():,} ({teams_with_size['IsSolo'].mean()*100:.1f}%)\")\n",
    "print(f\"Multi-member teams: {(~teams_with_size['IsSolo']).sum():,} ({(~teams_with_size['IsSolo']).mean()*100:.1f}%)\")\n",
    "print(f\"\\nTeam size statistics:\")\n",
    "print(teams_with_size['TeamSize'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "067b229f8750433c",
   "metadata": {},
   "source": [
    "# team size distribution (capped at 20 for readability)\n",
    "size_counts = teams_with_size['TeamSize'].value_counts().sort_index().reset_index()\n",
    "size_counts.columns = ['TeamSize', 'Count']\n",
    "size_counts = size_counts[size_counts['TeamSize'] <= 20]\n",
    "\n",
    "fig = px.bar(size_counts, x='TeamSize', y='Count',\n",
    "             title='Distribution of Team Sizes Across All Competitions',\n",
    "             labels={'TeamSize': 'Number of Team Members', 'Count': 'Number of Teams'},\n",
    "             color_discrete_sequence=['#F67280'])\n",
    "fig.update_layout(width=1000, height=500, title_x=0.5)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d39f4631c7244733",
   "metadata": {},
   "source": [
    "# medal distribution: solo vs team by size category\n",
    "teams_with_medal = teams_with_size[teams_with_size['Medal'].notna() & (teams_with_size['Medal'] != '')]\n",
    "teams_with_medal = teams_with_medal.copy()\n",
    "teams_with_medal['SizeCategory'] = pd.cut(\n",
    "    teams_with_medal['TeamSize'],\n",
    "    bins=[0, 1, 2, 3, 5, 100],\n",
    "    labels=['Solo', 'Duo', 'Trio', '4-5', '6+'])\n",
    "\n",
    "medal_labels = {'1': 'Gold', '2': 'Silver', '3': 'Bronze'}\n",
    "teams_with_medal['MedalName'] = teams_with_medal['Medal'].astype(str).map(medal_labels)\n",
    "\n",
    "medal_by_size = teams_with_medal.groupby(['SizeCategory', 'MedalName']).size().reset_index(name='Count')\n",
    "\n",
    "fig = px.bar(medal_by_size, x='SizeCategory', y='Count', color='MedalName',\n",
    "             title='Medal Distribution by Team Size Category',\n",
    "             labels={'SizeCategory': 'Team Size', 'Count': 'Number of Medals'},\n",
    "             color_discrete_map={'Gold': '#FFD700', 'Silver': '#C0C0C0', 'Bronze': '#CD7F32'},\n",
    "             barmode='group',\n",
    "             category_orders={'MedalName': ['Gold', 'Silver', 'Bronze']})\n",
    "fig.update_layout(width=1000, height=500, title_x=0.5)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62ff6157bcc349ae",
   "metadata": {},
   "source": [
    "# medal RATE by team size — what proportion of teams earn any medal?\n",
    "medal_rate = teams_with_size.copy()\n",
    "medal_rate['HasMedal'] = medal_rate['Medal'].notna() & (medal_rate['Medal'] != '')\n",
    "medal_rate['SizeCategory'] = pd.cut(\n",
    "    medal_rate['TeamSize'],\n",
    "    bins=[0, 1, 2, 3, 5, 100],\n",
    "    labels=['Solo', 'Duo', 'Trio', '4-5', '6+'])\n",
    "\n",
    "rate_by_size = medal_rate.groupby('SizeCategory', observed=True).agg(\n",
    "    TotalTeams=('HasMedal', 'count'),\n",
    "    MedalTeams=('HasMedal', 'sum')\n",
    ").reset_index()\n",
    "rate_by_size['MedalRate'] = (rate_by_size['MedalTeams'] / rate_by_size['TotalTeams'] * 100).round(2)\n",
    "\n",
    "fig = px.bar(rate_by_size, x='SizeCategory', y='MedalRate',\n",
    "             title='Medal Rate by Team Size (% of Teams Earning Any Medal)',\n",
    "             labels={'SizeCategory': 'Team Size', 'MedalRate': 'Medal Rate (%)'},\n",
    "             color_discrete_sequence=['#355C7D'],\n",
    "             text='MedalRate')\n",
    "fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')\n",
    "fig.update_layout(width=1000, height=500, title_x=0.5)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a84dbde9ec7442a2",
   "metadata": {},
   "source": [
    "# solo vs team participation over time\n",
    "teams_with_comp = teams_with_size.merge(\n",
    "    competitions[['Id', 'EnabledDate']], left_on='CompetitionId', right_on='Id', suffixes=('', '_comp'))\n",
    "teams_with_comp['Year'] = teams_with_comp['EnabledDate'].dt.year\n",
    "\n",
    "yearly_solo = teams_with_comp.groupby('Year').agg(\n",
    "    Total=('IsSolo', 'count'),\n",
    "    SoloCount=('IsSolo', 'sum')\n",
    ").reset_index()\n",
    "yearly_solo['SoloPct'] = (yearly_solo['SoloCount'] / yearly_solo['Total'] * 100).round(1)\n",
    "yearly_solo['TeamPct'] = (100 - yearly_solo['SoloPct']).round(1)\n",
    "yearly_solo = yearly_solo[yearly_solo['Year'].between(2010, 2025)]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=yearly_solo['Year'], y=yearly_solo['SoloPct'],\n",
    "                     name='Solo', marker_color='#355C7D'))\n",
    "fig.add_trace(go.Bar(x=yearly_solo['Year'], y=yearly_solo['TeamPct'],\n",
    "                     name='Team', marker_color='#F67280'))\n",
    "fig.update_layout(barmode='stack',\n",
    "                  title='Solo vs. Team Participation Over Time',\n",
    "                  xaxis_title='Year', yaxis_title='Percentage (%)',\n",
    "                  width=1000, height=500, title_x=0.5)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3e8ff8f9a874aae",
   "metadata": {},
   "source": [
    "### 3.D) Professional vs. Amateur Competitors (Research Question 4)\n",
    "\n",
    "> *Are these winners professionals or amateurs in the fields of machine learning and data science?*\n",
    "\n",
    "- Kaggle assigns each user a **Performance Tier** (Novice → Contributor → Expert → Master → Grandmaster)\n",
    "- We use this as a proxy for experience level and examine how tier correlates with competition success."
   ]
  },
  {
   "cell_type": "code",
   "id": "76096221887d4b7b",
   "metadata": {},
   "source": [
    "# performance tier distribution\n",
    "tier_labels = {0: 'Novice', 1: 'Contributor', 2: 'Expert', 3: 'Master', 4: 'Grandmaster', 5: 'Staff'}\n",
    "tier_order = ['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster', 'Staff']\n",
    "users['TierLabel'] = users['PerformanceTier'].map(tier_labels)\n",
    "\n",
    "tier_counts = users['TierLabel'].value_counts().reset_index()\n",
    "tier_counts.columns = ['Tier', 'Count']\n",
    "\n",
    "fig = px.bar(tier_counts, x='Tier', y='Count',\n",
    "             title='Distribution of User Performance Tiers',\n",
    "             color='Tier',\n",
    "             color_discrete_map={\n",
    "                 'Novice': '#B0BEC5', 'Contributor': '#81C784', 'Expert': '#4FC3F7',\n",
    "                 'Master': '#BA68C8', 'Grandmaster': '#FFD54F', 'Staff': '#FF8A65'},\n",
    "             category_orders={'Tier': tier_order})\n",
    "fig.update_layout(width=1000, height=500, title_x=0.5, showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "print(\"Performance Tier Counts:\")\n",
    "for tier in tier_order:\n",
    "    c = tier_counts[tier_counts['Tier'] == tier]['Count'].values\n",
    "    if len(c) > 0:\n",
    "        print(f\"  {tier:15s}: {c[0]:>10,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "afab07e0244c4277",
   "metadata": {},
   "source": [
    "# competition achievements by performance tier\n",
    "ua_with_tier = user_achievements.merge(\n",
    "    users[['Id', 'PerformanceTier', 'TierLabel']], left_on='UserId', right_on='Id', how='left')\n",
    "\n",
    "comp_achievers = ua_with_tier['TierLabel'].value_counts().reset_index()\n",
    "comp_achievers.columns = ['Tier', 'Count']\n",
    "\n",
    "fig = px.pie(comp_achievers, names='Tier', values='Count',\n",
    "             title='Performance Tier Distribution of Users with Competition Achievements',\n",
    "             color='Tier',\n",
    "             color_discrete_map={\n",
    "                 'Novice': '#B0BEC5', 'Contributor': '#81C784', 'Expert': '#4FC3F7',\n",
    "                 'Master': '#BA68C8', 'Grandmaster': '#FFD54F', 'Staff': '#FF8A65'})\n",
    "fig.update_layout(width=800, height=500, title_x=0.5)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18c46923b4134306",
   "metadata": {},
   "source": [
    "# total competition medals (gold/silver/bronze) by performance tier\n",
    "medal_by_tier = ua_with_tier.groupby('TierLabel').agg(\n",
    "    TotalGold=('TotalGold', 'sum'),\n",
    "    TotalSilver=('TotalSilver', 'sum'),\n",
    "    TotalBronze=('TotalBronze', 'sum'),\n",
    "    UserCount=('UserId', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "tier_rank_order = ['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster']\n",
    "medal_by_tier = medal_by_tier[medal_by_tier['TierLabel'].isin(tier_rank_order)]\n",
    "medal_by_tier['TierLabel'] = pd.Categorical(\n",
    "    medal_by_tier['TierLabel'], categories=tier_rank_order, ordered=True)\n",
    "medal_by_tier = medal_by_tier.sort_values('TierLabel')\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=medal_by_tier['TierLabel'], y=medal_by_tier['TotalGold'],\n",
    "                     name='Gold', marker_color='#FFD700'))\n",
    "fig.add_trace(go.Bar(x=medal_by_tier['TierLabel'], y=medal_by_tier['TotalSilver'],\n",
    "                     name='Silver', marker_color='#C0C0C0'))\n",
    "fig.add_trace(go.Bar(x=medal_by_tier['TierLabel'], y=medal_by_tier['TotalBronze'],\n",
    "                     name='Bronze', marker_color='#CD7F32'))\n",
    "fig.update_layout(barmode='group',\n",
    "                  title='Total Competition Medals by Performance Tier',\n",
    "                  xaxis_title='Performance Tier', yaxis_title='Total Medals',\n",
    "                  width=1000, height=500, title_x=0.5)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fd6bc05b8bc4b07",
   "metadata": {},
   "source": [
    "# average medals PER USER by tier (more meaningful than raw totals)\n",
    "medal_by_tier['AvgGold'] = (medal_by_tier['TotalGold'] / medal_by_tier['UserCount']).round(3)\n",
    "medal_by_tier['AvgSilver'] = (medal_by_tier['TotalSilver'] / medal_by_tier['UserCount']).round(3)\n",
    "medal_by_tier['AvgBronze'] = (medal_by_tier['TotalBronze'] / medal_by_tier['UserCount']).round(3)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=medal_by_tier['TierLabel'], y=medal_by_tier['AvgGold'],\n",
    "                     name='Avg Gold', marker_color='#FFD700'))\n",
    "fig.add_trace(go.Bar(x=medal_by_tier['TierLabel'], y=medal_by_tier['AvgSilver'],\n",
    "                     name='Avg Silver', marker_color='#C0C0C0'))\n",
    "fig.add_trace(go.Bar(x=medal_by_tier['TierLabel'], y=medal_by_tier['AvgBronze'],\n",
    "                     name='Avg Bronze', marker_color='#CD7F32'))\n",
    "fig.update_layout(barmode='group',\n",
    "                  title='Average Competition Medals Per User by Performance Tier',\n",
    "                  xaxis_title='Performance Tier', yaxis_title='Avg Medals per User',\n",
    "                  width=1000, height=500, title_x=0.5)\n",
    "fig.show()\n",
    "\n",
    "print(\"Medals per user by tier:\")\n",
    "print(medal_by_tier[['TierLabel', 'UserCount', 'AvgGold', 'AvgSilver', 'AvgBronze']].to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9d59773a40e430a",
   "metadata": {},
   "source": [
    "# what performance tier are team leaders of MEDAL-WINNING teams?\n",
    "medal_teams = teams_with_size[teams_with_size['Medal'].notna() & (teams_with_size['Medal'] != '')]\n",
    "leader_tiers = medal_teams.merge(\n",
    "    users[['Id', 'TierLabel']], left_on='TeamLeaderId', right_on='Id', how='left')\n",
    "\n",
    "leader_tiers['MedalName'] = leader_tiers['Medal'].astype(str).map(\n",
    "    {'1': 'Gold', '2': 'Silver', '3': 'Bronze'})\n",
    "leader_tier_medal = leader_tiers.groupby(['TierLabel', 'MedalName']).size().reset_index(name='Count')\n",
    "\n",
    "fig = px.bar(leader_tier_medal, x='TierLabel', y='Count', color='MedalName',\n",
    "             title='Medal-Winning Team Leader Performance Tiers',\n",
    "             labels={'TierLabel': 'Leader Performance Tier', 'Count': 'Medal-Winning Teams'},\n",
    "             color_discrete_map={'Gold': '#FFD700', 'Silver': '#C0C0C0', 'Bronze': '#CD7F32'},\n",
    "             barmode='group',\n",
    "             category_orders={\n",
    "                 'TierLabel': ['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster'],\n",
    "                 'MedalName': ['Gold', 'Silver', 'Bronze']})\n",
    "fig.update_layout(width=1000, height=500, title_x=0.5)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "msc7vxctpie",
   "source": "#### 3.D.ii) ***Competition Categories Where Amateurs Succeed***\n\nWhich competition tags (subject areas) give Novice and Contributor-tier users the best chance at earning a medal? This identifies the most accessible niches for newcomers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4goemh7q8dy",
   "source": "# Join team leaders to their performance tier and competition tags\nteam_leader_tier = teams.merge(\n    users[['Id', 'PerformanceTier']], left_on='TeamLeaderId', right_on='Id', suffixes=('', '_user'))\nteam_leader_tier['HasMedal'] = team_leader_tier['Medal'].notna() & (team_leader_tier['Medal'] != '')\nteam_leader_tier['IsAmateur'] = team_leader_tier['PerformanceTier'].isin([0, 1])  # Novice, Contributor\n\n# Get competition tag names\ncomp_tag_names = comp_tags.merge(tags[['Id', 'Name']], left_on='TagId', right_on='Id', suffixes=('', '_tag'))\n\n# Join teams to competition tags\nteam_tags = team_leader_tier.merge(\n    comp_tag_names[['CompetitionId', 'Name']], on='CompetitionId', how='inner')\n\n# Amateur medal rate per tag (require >=100 amateur participants for significance)\namateur_by_tag = team_tags[team_tags['IsAmateur']].groupby('Name').agg(\n    AmateurTeams=('HasMedal', 'count'),\n    AmateurMedals=('HasMedal', 'sum')\n).reset_index()\namateur_by_tag['MedalRate'] = (amateur_by_tag['AmateurMedals'] / amateur_by_tag['AmateurTeams'] * 100).round(2)\namateur_by_tag = amateur_by_tag[amateur_by_tag['AmateurTeams'] >= 100].sort_values(\n    'MedalRate', ascending=False).head(20)\n\nfig = px.bar(amateur_by_tag, x='MedalRate', y='Name', orientation='h',\n             title='Top 20 Competition Categories by Amateur (Novice/Contributor) Medal Rate',\n             labels={'Name': 'Competition Tag', 'MedalRate': 'Amateur Medal Rate (%)'},\n             color='MedalRate',\n             color_continuous_scale='YlOrRd',\n             hover_data={'AmateurTeams': ':,', 'AmateurMedals': ':,'})\nfig.update_layout(width=1000, height=600, title_x=0.5,\n                  yaxis={'categoryorder': 'total ascending'},\n                  coloraxis_showscale=False)\nfig.show()\n\n# Also show the comparison: amateur vs professional medal rate per tag\npro_by_tag = team_tags[~team_tags['IsAmateur']].groupby('Name').agg(\n    ProTeams=('HasMedal', 'count'),\n    ProMedals=('HasMedal', 'sum')\n).reset_index()\npro_by_tag['ProMedalRate'] = (pro_by_tag['ProMedals'] / pro_by_tag['ProTeams'] * 100).round(2)\n\ncomparison = amateur_by_tag.merge(pro_by_tag[['Name', 'ProMedalRate']], on='Name', how='left')\ncomparison['AmateurAdvantageRatio'] = (comparison['MedalRate'] / comparison['ProMedalRate']).round(2)\ncomparison = comparison.sort_values('MedalRate', ascending=False)\n\nprint(\"Amateur vs. Professional medal rates (top 20 categories for amateurs):\")\nprint(comparison[['Name', 'AmateurTeams', 'MedalRate', 'ProMedalRate', 'AmateurAdvantageRatio']]\n      .rename(columns={'MedalRate': 'Amateur%', 'ProMedalRate': 'Pro%', 'AmateurAdvantageRatio': 'Ratio'})\n      .to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2swo0kkmm99",
   "source": "#### 3.D.iii) ***Profiling Gold Medals Won by Novice/Contributor-Tier Users***\n\nDo amateurs ever win Gold? If so, what do those competitions look like — how large are they, what categories, are they kernel-only? This gives a concrete profile of \"winnable\" competitions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3sgypsfb6fg",
   "source": "# Find Gold medal teams led by Novice or Contributor-tier users\ntier_labels_map = {0: 'Novice', 1: 'Contributor', 2: 'Expert', 3: 'Master', 4: 'Grandmaster', 5: 'Staff'}\n\ngold_teams = teams[teams['Medal'].notna() & (teams['Medal'] != '')].copy()\ngold_teams['MedalNum'] = pd.to_numeric(gold_teams['Medal'], errors='coerce')\ngold_teams = gold_teams[gold_teams['MedalNum'] == 1]  # Gold only\n\ngold_leaders = gold_teams.merge(\n    users[['Id', 'PerformanceTier', 'UserName']], left_on='TeamLeaderId', right_on='Id', suffixes=('', '_user'))\ngold_leaders['TierLabel'] = gold_leaders['PerformanceTier'].map(tier_labels_map)\n\n# Filter to amateur Gold winners (Novice + Contributor)\namateur_golds = gold_leaders[gold_leaders['PerformanceTier'].isin([0, 1])].copy()\namateur_golds = amateur_golds.merge(\n    competitions[['Id', 'Title', 'TotalCompetitors', 'OnlyAllowKernelSubmissions', 'RewardType']],\n    left_on='CompetitionId', right_on='Id', suffixes=('', '_comp'))\n\nprint(f\"Total Gold medals across all competitions: {len(gold_leaders):,}\")\nprint(f\"Gold medals won by Novice/Contributor leaders: {len(amateur_golds):,} \"\n      f\"({len(amateur_golds)/len(gold_leaders)*100:.1f}%)\\n\")\n\n# Profile these competitions\nprint(\"=== Profile of Competitions Where Amateurs Won Gold ===\")\nprint(f\"  Median competition size: {amateur_golds['TotalCompetitors'].median():.0f} competitors\")\nprint(f\"  Mean competition size:   {amateur_golds['TotalCompetitors'].mean():.0f} competitors\")\nprint(f\"  Kernel-only: {amateur_golds['OnlyAllowKernelSubmissions'].sum()} / {len(amateur_golds)} \"\n      f\"({amateur_golds['OnlyAllowKernelSubmissions'].mean()*100:.1f}%)\")\n\n# Competition size distribution for amateur Golds\nfig = px.histogram(amateur_golds, x='TotalCompetitors', nbins=30,\n                   title='Competition Size Distribution Where Amateurs (Novice/Contributor) Won Gold',\n                   labels={'TotalCompetitors': 'Total Competitors'},\n                   color_discrete_sequence=['#FFD700'])\nfig.update_layout(width=1000, height=400, title_x=0.5)\nfig.show()\n\n# Tier breakdown of all Gold winners for context\ngold_tier_counts = gold_leaders['TierLabel'].value_counts().reset_index()\ngold_tier_counts.columns = ['Tier', 'GoldCount']\n\nfig = px.pie(gold_tier_counts, names='Tier', values='GoldCount',\n             title='Performance Tier of All Gold Medal Team Leaders',\n             color='Tier',\n             color_discrete_map={\n                 'Novice': '#B0BEC5', 'Contributor': '#81C784', 'Expert': '#4FC3F7',\n                 'Master': '#BA68C8', 'Grandmaster': '#FFD54F', 'Staff': '#FF8A65'})\nfig.update_layout(width=800, height=500, title_x=0.5)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "528aa761c9154827",
   "metadata": {},
   "source": [
    "### 3.E) Cross-Cutting Analysis & Synthesis\n",
    "\n",
    "Combining findings across the research questions to identify broader patterns:\n",
    "- what language + hardware combinations appear in competition kernels\n",
    "- how kernel-only competitions differ\n",
    "- a high-level summary of key findings."
   ]
  },
  {
   "cell_type": "code",
   "id": "325eff6ff4ec4b7e",
   "metadata": {},
   "source": [
    "# top language + accelerator combinations in competition kernels\n",
    "comp_kv_full = comp_kernel_versions.merge(\n",
    "    kernel_languages, left_on='ScriptLanguageId', right_on='Id', suffixes=('', '_lang'))\n",
    "comp_kv_full = comp_kv_full.merge(\n",
    "    accelerator_types, left_on='AcceleratorTypeId', right_on='Id', suffixes=('', '_accel'))\n",
    "\n",
    "lang_accel = comp_kv_full.groupby(['DisplayName', 'Label']).size().reset_index(name='Count')\n",
    "lang_accel = lang_accel.sort_values('Count', ascending=False).head(15)\n",
    "lang_accel['Combo'] = lang_accel['DisplayName'] + ' + ' + lang_accel['Label']\n",
    "\n",
    "fig = px.bar(lang_accel, x='Count', y='Combo', orientation='h',\n",
    "             title='Top 15 Language + Accelerator Combinations in Competition Kernels',\n",
    "             labels={'Combo': 'Language + Accelerator'},\n",
    "             color_discrete_sequence=['#6C5B7B'])\n",
    "fig.update_layout(width=1000, height=500, title_x=0.5,\n",
    "                  yaxis={'categoryorder': 'total ascending'})\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14fe2cdc349b4e1f",
   "metadata": {},
   "source": [
    "# kernel-only competitions vs. open-submission competitions\n",
    "kernel_only = competitions[competitions['OnlyAllowKernelSubmissions'] == True]\n",
    "non_kernel_only = competitions[competitions['OnlyAllowKernelSubmissions'] == False]\n",
    "\n",
    "print(f\"Kernel-only competitions: {len(kernel_only):,}\")\n",
    "print(f\"Open-submission competitions: {len(non_kernel_only):,}\")\n",
    "print()\n",
    "print(\"=== Kernel-Only Competition Stats ===\")\n",
    "print(kernel_only[['TotalTeams', 'TotalCompetitors', 'TotalSubmissions']].describe().round(1))\n",
    "print()\n",
    "print(\"=== Open-Submission Competition Stats ===\")\n",
    "print(non_kernel_only[['TotalTeams', 'TotalCompetitors', 'TotalSubmissions']].describe().round(1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bjj1yn3rysm",
   "source": "#### 3.E.iii) ***Do Kernel-Only Competitions Level the Playing Field for Amateurs?***\n\nKernel-only competitions force all participants onto Kaggle's provided hardware, eliminating the hardware advantage that professionals may have. Do amateurs medal at higher rates in these competitions compared to open-submission ones?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "t0s7dtktsgd",
   "source": "# Compare amateur medal rates: kernel-only vs. open-submission competitions\nteams_konly = teams.merge(\n    competitions[['Id', 'OnlyAllowKernelSubmissions']],\n    left_on='CompetitionId', right_on='Id', suffixes=('', '_comp'))\nteams_konly = teams_konly.merge(\n    users[['Id', 'PerformanceTier']], left_on='TeamLeaderId', right_on='Id', suffixes=('', '_user'))\n\nteams_konly['HasMedal'] = teams_konly['Medal'].notna() & (teams_konly['Medal'] != '')\nteams_konly['CompType'] = teams_konly['OnlyAllowKernelSubmissions'].map(\n    {True: 'Kernel-Only', False: 'Open Submission'})\n\ntier_labels_full = {0: 'Novice', 1: 'Contributor', 2: 'Expert', 3: 'Master', 4: 'Grandmaster'}\nteams_konly['TierLabel'] = teams_konly['PerformanceTier'].map(tier_labels_full)\nteams_konly = teams_konly[teams_konly['TierLabel'].notna()]  # exclude Staff\n\n# Medal rate by tier and competition type\nmedal_by_type_tier = teams_konly.groupby(['CompType', 'TierLabel']).agg(\n    Total=('HasMedal', 'count'),\n    Medals=('HasMedal', 'sum')\n).reset_index()\nmedal_by_type_tier['MedalRate'] = (medal_by_type_tier['Medals'] / medal_by_type_tier['Total'] * 100).round(3)\n\ntier_order = ['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster']\n\nfig = px.bar(medal_by_type_tier, x='TierLabel', y='MedalRate', color='CompType',\n             barmode='group',\n             title='Medal Rate by Experience Level: Kernel-Only vs. Open Submission',\n             labels={'TierLabel': 'Performance Tier', 'MedalRate': 'Medal Rate (%)', 'CompType': 'Competition Type'},\n             category_orders={'TierLabel': tier_order},\n             color_discrete_map={'Kernel-Only': '#20BEFF', 'Open Submission': '#F67280'})\nfig.update_layout(width=1000, height=500, title_x=0.5)\nfig.show()\n\n# Print the key comparison for amateurs\nprint(\"Medal rates — Kernel-Only vs. Open Submission:\\n\")\nfor tier in tier_order:\n    row_k = medal_by_type_tier[(medal_by_type_tier['TierLabel'] == tier) &\n                                (medal_by_type_tier['CompType'] == 'Kernel-Only')]\n    row_o = medal_by_type_tier[(medal_by_type_tier['TierLabel'] == tier) &\n                                (medal_by_type_tier['CompType'] == 'Open Submission')]\n    k_rate = row_k['MedalRate'].values[0] if len(row_k) > 0 else 0\n    o_rate = row_o['MedalRate'].values[0] if len(row_o) > 0 else 0\n    diff = k_rate - o_rate\n    print(f\"  {tier:15s}  Kernel-Only: {k_rate:.3f}%  |  Open: {o_rate:.3f}%  |  Diff: {diff:+.3f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.F) EDA Summary",
   "id": "7f34c7aa599059f1"
  },
  {
   "cell_type": "code",
   "id": "38fefbef14ab4f8f",
   "metadata": {},
   "source": [
    "# ── EDA SUMMARY ──\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPLORATORY DATA ANALYSIS — KEY FINDINGS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Languages\n",
    "total_kvs = len(kernel_versions)\n",
    "python_ids = kernel_languages[kernel_languages['DisplayName'] == 'Python']['Id'].tolist()\n",
    "python_kvs = len(kernel_versions[kernel_versions['ScriptLanguageId'].isin(python_ids)])\n",
    "print(f\"\\n1. LANGUAGES (Research Q1):\")\n",
    "print(f\"   Python (scripts + notebooks): {python_kvs:,} / {total_kvs:,} ({python_kvs/total_kvs*100:.1f}%)\")\n",
    "r_ids = kernel_languages[kernel_languages['DisplayName'] == 'R']['Id'].tolist()\n",
    "r_kvs = len(kernel_versions[kernel_versions['ScriptLanguageId'].isin(r_ids)])\n",
    "print(f\"   R (scripts + notebooks):      {r_kvs:,} / {total_kvs:,} ({r_kvs/total_kvs*100:.1f}%)\")\n",
    "\n",
    "# 2. Solo vs team\n",
    "total_teams_count = len(teams_with_size)\n",
    "solo_count = int(teams_with_size['IsSolo'].sum())\n",
    "print(f\"\\n2. INDIVIDUALS vs TEAMS (Research Q3):\")\n",
    "print(f\"   Solo competitors: {solo_count:,} / {total_teams_count:,} ({solo_count/total_teams_count*100:.1f}%)\")\n",
    "\n",
    "# 3. Hardware\n",
    "total_with_accel = int((kernel_versions['AcceleratorTypeId'] != 0).sum())\n",
    "print(f\"\\n3. HARDWARE (Research Q5):\")\n",
    "print(f\"   Kernels using any accelerator: {total_with_accel:,} / {total_kvs:,} ({total_with_accel/total_kvs*100:.1f}%)\")\n",
    "print(f\"   Kernels with NO accelerator:   {total_kvs - total_with_accel:,} / {total_kvs:,} ({(total_kvs - total_with_accel)/total_kvs*100:.1f}%)\")\n",
    "\n",
    "# 4. Experience levels\n",
    "total_users = len(users)\n",
    "novice_count = int((users['PerformanceTier'] == 0).sum())\n",
    "gm_count = int((users['PerformanceTier'] == 4).sum())\n",
    "master_count = int((users['PerformanceTier'] == 3).sum())\n",
    "print(f\"\\n4. EXPERIENCE LEVELS (Research Q4):\")\n",
    "print(f\"   Novice users:   {novice_count:>10,} / {total_users:,} ({novice_count/total_users*100:.1f}%)\")\n",
    "print(f\"   Grandmasters:   {gm_count:>10,} / {total_users:,} ({gm_count/total_users*100:.1f}%)\")\n",
    "print(f\"   Masters:        {master_count:>10,} / {total_users:,} ({master_count/total_users*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}